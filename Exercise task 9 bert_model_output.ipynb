{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_model_output.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Deep-Learning-in-Human-Language-Technology/blob/main/Exercise%20task%209%20bert_model_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic BERT operations\n"
      ],
      "metadata": {
        "id": "U2k3QEnj0hTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YuY096pr0DK_"
      },
      "outputs": [],
      "source": [
        "!pip3 -q install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import torch"
      ],
      "metadata": {
        "id": "WMej7EPhAFRt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\") #you can also use the trusty \"TurkuNLP/bert-base-finnish-cased-v1\""
      ],
      "metadata": {
        "id": "SItSFN_-0sTU",
        "outputId": "3d9e51c4-c24b-4f9b-bed5-fd5dcdd23aa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts='As a Lord of the Rings fan, I was eagerly awaiting the origin stories of Middle-earth. Of course, I have high [MASK] after Lord of the Rings, which is close to perfection in terms of time and fiction. Because they have a considerable budget and opportunities, that\\'s why I gave my points by watching the first episode right away.'"
      ],
      "metadata": {
        "id": "lSCB7bH9pMfo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be running the model directly, so let's use return_tensors=\"pt\" to get torch tensors rather than Python lists\n",
        "#texts=[\"Dogs like to [MASK] cats. They taste good.\",\"Bad joke!\"]\n",
        "t=tokenizer(texts,padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(\"Input ids\",t[\"input_ids\"])\n",
        "print(\"Token type ids\",t[\"token_type_ids\"])\n",
        "print(\"Attention mask\",t[\"attention_mask\"])"
      ],
      "metadata": {
        "id": "G3gKwiAf1Fg_",
        "outputId": "4e7a4fdb-f3c0-4b9f-ba28-3e4066a98804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input ids tensor([[  101,  1249,   170,  2188,  1104,  1103, 22518,  5442,   117,   146,\n",
            "          1108, 19379, 16794,  1103,  4247,  2801,  1104,  3089,   118,  4033,\n",
            "           119,  2096,  1736,   117,   146,  1138,  1344,   103,  1170,  2188,\n",
            "          1104,  1103, 22518,   117,  1134,  1110,  1601,  1106, 17900,  1107,\n",
            "          2538,  1104,  1159,  1105,  4211,   119,  2279,  1152,  1138,   170,\n",
            "          5602,  4788,  1105,  6305,   117,  1115,   112,   188,  1725,   146,\n",
            "          1522,  1139,  1827,  1118,  2903,  1103,  1148,  2004,  1268,  1283,\n",
            "           119,   102]])\n",
            "Token type ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "Attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is what the first sequence looks like\n",
        "tokenizer.decode(t[\"input_ids\"][0])"
      ],
      "metadata": {
        "id": "Hed6oOCO1Z1N",
        "outputId": "6ebf5f37-0ea2-4137-bc59-e66d0b031926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[CLS] As a Lord of the Rings fan, I was eagerly awaiting the origin stories of Middle - earth. Of course, I have high [MASK] after Lord of the Rings, which is close to perfection in terms of time and fiction. Because they have a considerable budget and opportunities, that's why I gave my points by watching the first episode right away. [SEP]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT: bare model\n",
        "* How to use the bare model\n",
        "* What does it give us?"
      ],
      "metadata": {
        "id": "tGout3eyB0qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert=transformers.AutoModel.from_pretrained(\"bert-base-cased\") #\"TurkuNLP/bert-base-finnish-cased-v1\" if you run this in Finnish\n"
      ],
      "metadata": {
        "id": "dF6eNBzO16Fq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* in torch the model's forward() function tends to be mapped to `__call__()` i.e. it is used when you call the model as if it were a function\n"
      ],
      "metadata": {
        "id": "cfCp49YDCPlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_out=bert(\n",
        "    input_ids=t[\"input_ids\"],\n",
        "    attention_mask=t[\"attention_mask\"],\n",
        "    token_type_ids=t[\"token_type_ids\"])\n",
        "#an easy way to say the above would be bert(**t)\n"
      ],
      "metadata": {
        "id": "KkunPLnt2QP0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "that's it, this is how you call BERT, now let's see what it gave us (not hard to figure out it is really a dictionary)"
      ],
      "metadata": {
        "id": "w93TRT3rCloK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_out.keys()"
      ],
      "metadata": {
        "id": "CvGUp99n3e9x",
        "outputId": "4c9700be-12e0-40e2-c740-0343654e2832",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['last_hidden_state', 'pooler_output'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* last_hidden_state: the last layer of the encoder\n",
        "* pooler_output: the `tanh` layer on top of `[CLS]`"
      ],
      "metadata": {
        "id": "m6X-qckrC1VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before you run this, stop to think:\n",
        "# What will the shape be? How many dimensions? 1? 2? 3? more? And their approximate sizes?\n",
        "# make a guess, see if it matches\n",
        "bert_out.last_hidden_state.shape"
      ],
      "metadata": {
        "id": "vlaUdarn3Yj7",
        "outputId": "d7b443bf-9880-4520-dc04-cb6b8aab9e92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 72, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And here? What will the shape be?\n",
        "bert_out.pooler_output.shape"
      ],
      "metadata": {
        "id": "qIyI5ZfG3sBj",
        "outputId": "835a9f8e-9258-49ce-e141-547642d8b926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT: masked language modelling output\n",
        "\n",
        "* Not much we can do with the above\n",
        "* But BERT is trained to predict masked words, let's try!"
      ],
      "metadata": {
        "id": "AXhX93tbD2tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Have a look at HuggingFace automodels documentation to see what types of automodels there are\n",
        "bert=transformers.AutoModelForPreTraining.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "iQy5gL0Z4VFW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tell the model it is not really being trained (disables dropout for example)\n",
        "# I do not think this is needed but am playing it safe, the docs say it is put to eval mode upon load: https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained.config\n",
        "bert=bert.eval()"
      ],
      "metadata": {
        "id": "RAQmsa5b41sI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can again run the model, and we will see the output is quite different!"
      ],
      "metadata": {
        "id": "LuFpVyBpFXjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_out=bert(**t)\n",
        "bert_out.keys()"
      ],
      "metadata": {
        "id": "HzHG0jn44-PL",
        "outputId": "15194bf9-1963-46c6-9e06-e0747cfec732",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['prediction_logits', 'seq_relationship_logits'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What are these? https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#transformers.BertForPreTraining\n",
        "#What do you think these shapes will be?\n",
        "print(\"Logits\",bert_out[\"prediction_logits\"].shape)\n",
        "print(\"Seq relationship logits\",bert_out[\"seq_relationship_logits\"].shape)"
      ],
      "metadata": {
        "id": "IYEFMKuK5A8-",
        "outputId": "c186ce56-c75c-4b81-93a0-7947f494dafc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits torch.Size([1, 72, 28996])\n",
            "Seq relationship logits torch.Size([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cross-check\n",
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "8avjXIqT5L9W",
        "outputId": "6c560528-65b5-4594-b145-af431ce3bb5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28996"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "...now let's see how well this works for the masked word prediction...\n",
        "* we need to find the most likely predicted words\n",
        "* which can be achieved by arg-sorting the predictions and picking top N words\n",
        "* this is easy and we have done this kind of stuff before\n",
        "* now let's try straight in torch without a roundtrip to numpy"
      ],
      "metadata": {
        "id": "XtSGBan5QaXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = bert_out[\"prediction_logits\"]\n",
        "print(predictions.shape)\n",
        "top20=torch.argsort(predictions,dim=2,descending=True)[:,:,:20] #why dim=2? what does [:,:,:20] do?\n",
        "print(top20)"
      ],
      "metadata": {
        "id": "t8lxob7T8sIL",
        "outputId": "127ff200-c64e-4dc7-ed1c-f9a699ecde89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 72, 28996])\n",
            "tensor([[[  119,   117,  1103,  ...,  1123,   113,   146],\n",
            "         [ 1249,   119,  1112,  ...,  1108,  1104,  1116],\n",
            "         [  170,  1126,   138,  ...,  1108,  1821,   188],\n",
            "         ...,\n",
            "         [ 1283, 11343,  1175,  ...,  1303,  1378,  1313],\n",
            "         [  119,   106,   132,  ...,  1272,  1139,  1362],\n",
            "         [  119,   132,  1232,  ...,  1570,  1111,   188]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0])\n",
        "\n",
        "print(\"Guesses:\",tokenizer.decode(top20[0,4]))"
      ],
      "metadata": {
        "id": "aHsdKBuU9H-z",
        "outputId": "e362687f-47e1-4c47-ec04-0ddc92acf8fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "Guesses: of to and Of the'from in with for. but bya at -, timer year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ...in one block..."
      ],
      "metadata": {
        "id": "M0iadvxCSRNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#texts=[\"Dogs like to [MASK] cats. They are cute.\"]\n",
        "texts='As a Lord of the Rings fan, I was eagerly awaiting the origin stories of Middle-earth. Of course, I have high [MASK] after Lord of the Rings, which is close to perfection in terms of time and fiction. Because they have a considerable budget and opportunities, that\\'s why I gave my points by watching the first episode right away.'\n",
        "t=tokenizer(texts,padding=True, truncation=True, return_tensors=\"pt\")\n",
        "bert_out=bert(**t)\n",
        "top20=torch.argsort(bert_out[\"prediction_logits\"],dim=2,descending=True)[:,:,:20]\n",
        "print(\"Guesses:\",tokenizer.decode(top20[0,4]))"
      ],
      "metadata": {
        "id": "4XhmgLMgRubI",
        "outputId": "14a4063e-5523-4823-b981-9ee8625e32d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guesses: of to and Of the'from in with for. but bya at -, timer year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(t)\n",
        "print(tokenizer.mask_token_id)"
      ],
      "metadata": {
        "id": "Nmi_zDnsTKbP",
        "outputId": "d598b6a7-0037-4338-d6bb-0917d7d4c35d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1249,   170,  2188,  1104,  1103, 22518,  5442,   117,   146,\n",
            "          1108, 19379, 16794,  1103,  4247,  2801,  1104,  3089,   118,  4033,\n",
            "           119,  2096,  1736,   117,   146,  1138,  1344,   103,  1170,  2188,\n",
            "          1104,  1103, 22518,   117,  1134,  1110,  1601,  1106, 17900,  1107,\n",
            "          2538,  1104,  1159,  1105,  4211,   119,  2279,  1152,  1138,   170,\n",
            "          5602,  4788,  1105,  6305,   117,  1115,   112,   188,  1725,   146,\n",
            "          1522,  1139,  1827,  1118,  2903,  1103,  1148,  2004,  1268,  1283,\n",
            "           119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASKS\n",
        "\n",
        "As an exercise, you can try to solve the following:\n",
        "\n",
        "1. How good is BERT at the masked language modelling (MLM) task? Feed random texts e.g. from the IMDB dataset, mask a random token at a time, and check: did BERT predict it correctly?\n",
        "2. If you did (1), can you answer did BERT predict it correctly in top-5?\n",
        "3. Try can you do better. Make yourself a program which picks random texts from one of the datasets we used in this course and produces two files: one with segments of texts with one [MASK] and one with the correct answers. Then try to guess the words without looking at the latter file and then compare your answers with the correct ones. How well did you do?\n"
      ],
      "metadata": {
        "id": "jZ3CmFs0VYed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a Lord of the Rings fan, I was eagerly awaiting the origin stories of Middle-earth. Of course, I have high **expectations** after Lord of the Rings, which is close to perfection in terms of time and fiction. Because they have a considerable budget and opportunities, that's why I gave my points by watching the first episode right away."
      ],
      "metadata": {
        "id": "Qm6uBgYDj3vv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E6mflU2sp-LA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comments: I have masked 'expectation' but the language model was not able to predict that at all."
      ],
      "metadata": {
        "id": "jUExixHZp-2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Choose a subset of comments (for simplicity, we will take 10 random samples from the test set)\n",
        "num_samples = 10\n",
        "comments = dataset['test']['text']\n",
        "random_comments = random.sample(comments, num_samples)\n",
        "\n",
        "# Prepare to display masked comments and answers\n",
        "masked_comments = []\n",
        "correct_answers = []\n",
        "\n",
        "for comment in random_comments:\n",
        "    # Tokenize the comment into words\n",
        "    words = comment.split()\n",
        "\n",
        "    # Randomly choose an index to mask (avoid masking the first and last word for better context)\n",
        "    if len(words) > 2:  # Ensure there's a token to mask\n",
        "        mask_index = random.randint(1, len(words) - 2)  # Avoid first and last word\n",
        "        correct_answer = words[mask_index]  # Store the correct answer\n",
        "\n",
        "        # Mask the selected word\n",
        "        words[mask_index] = \"[MASK]\"\n",
        "        masked_comment = \" \".join(words)\n",
        "\n",
        "        # Store results\n",
        "        masked_comments.append(masked_comment)\n",
        "        correct_answers.append(correct_answer)\n",
        "\n",
        "# Print masked comments and answers\n",
        "print(\"Masked Comments:\")\n",
        "for masked in masked_comments:\n",
        "    print(masked)\n",
        "\n",
        "print(\"\\nCorrect Answers:\")\n",
        "for answer in correct_answers:\n",
        "    print(answer)\n"
      ],
      "metadata": {
        "id": "xtw4xuCxqGBm",
        "outputId": "84ae9d36-68b1-4866-a2e3-f8cbef1bba2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masked Comments:\n",
            "So.. what can I tell you about this movie. If you cheated a lot in high school, you do recognize some cheattips...<br /><br />This is the best thing i can tell you about this film!<br /><br />If you like American-teen movies, maybe you also like it!<br /><br />But i don't see this kind of movies as something funny.. sorry to say but if you are older then 10 years, i shouldn't advise you to watch this.<br /><br />Because there is one shot with a [MASK] of beautiful women (girls.. in this movie) i'll give it a rate of: 2!<br /><br />so.. deal for yourself! good luck\n",
            "Remember when Rick Mercer was funny? 22 Minutes was a great show when Rick Mercer was on it and Made In Canada was a great show once too. Talking To Americans was such a funny special too. But like my friend said \"Rick Mercer woke up one day and wasn't funny any more\" I think that day was when Rick Mercer Report went on the air. What is the point of this show? Rick Mercer reads wacky fake headlines, shows pictures of bad sheds that people mail in and then spends about 20 minutes of the 30 minute show going somewhere and just talking to people hoping to say something witty or clever enough to get on TV and maybe even make somebody somewhere laugh. We're supposed to be interested in seeing Rick Mercer visit a gymnastics team and then try to do some of their moves, and then suck at it on purpose while trying desperately to be [MASK] Rick Mercer got old or just lost interest or just ain't funny any more. Even his classic rant bits have lost all their bite and humor. You can say that about CBC comedy in general though because how many years have they been sticking Air Farce on TV to deliver the same kinds of useless jokes?\n",
            "This movie was disappointing for at least one of two reasons. The suspense created disappeared because of horrible acting or lack of direction from the director.. I don't know.. it was like a tasty bubble gum that seemed to run out of flavor yet you continue to chew on it because it once tasted great. Like most thrillers The Hitchhiker had promise yet failed to deliver when it had me bright eyed and ready to turn the volume down(I [MASK] watching the movie alone.. in the dark) This so called thriller simply came apart like it was made of Lego transforming into something else. It simply ran out of gas and left me staring at a made-for-TV-like style movie with one exception.. it was probably rated-R.\n",
            "This film was shot in Randolph County in central North Carolina in 1968 when a film crew in the [MASK] was a rare thing. The locations were the municipalities of Liberty and Ramseur and the surrounding rural countryside. It is not a particularly good movie. It did have Merle Haggard and it brought life to the hinterlands for a few minutes.<br /><br />The plot is standard shootemup. The cinematography is that fuzzy stuff that came out of the late sixties and early seventies. The local folks were thrilled to be a part of the enterprise.<br /><br />If viewers have difficulty finding a copy of this film, a record copy is available in Asheboro, NC.<br /><br />Actors not credited include Ben Jones, Mimi Pravda, Tommy Hull, Bill Nunnery.\n",
            "Based on the true story of the FBIs [MASK] for those who were responsible for the bombing of the World Trade Center Building. A very good film that delves into the FBIs use of informants and how, possibly, the tragedy could have been avoided 7 of 10\n",
            "The Duke is a very silly film--a dog becoming a duke! But it's a very fun movie. It has some of those corny pranks [MASK] many kids movies have, but (thankfully!) no bodily function jokes, as so many animal movies feel compelled to have! Mostly, it's just dogs being dogs and people being. . . well, people. The 'good guys' are likeable and appealing. The 'bad guys' are ridiculous, and of course, the pun of many jokes. But there is something dignified about this movie, for even though it is silly, it's not out for every cheap laugh like \"Home Alone\" and others.<br /><br />Crocket, Simon and Copper do an excellent job playing Black and Tan Coonhound \"Hubert\" who becomes the Duke after his beloved owner, a real Duke, dies. For the most part, they just act like dogs, no 'talking,' or human-like emotions and attitudes. However, they do stereotype poodles, and Hubert does fall for her, just because she's a poodle. Come on! These are dogs--they have a different view of beauty!!!<br /><br />Overall, charming, fun and enjoyable.\n",
            "It was September 2003 that I heard the BBC were going to resurrect DOCTOR WHO and make it \" Bigger and better \" but I'd heard these rumours in the press before and thought that's all they were - Rumours . But it was then mentioned that Russell T Davies was going to executively produce and write the show and then one Saturday afternoon in March 2004 Channel 4 news interviewed the actor cast in the title role - Christopher Eccleston . Yes that Christopher Eccleston an actor I've always been impressed by since watching his film debut in LET HIM HAVE IT and if he was getting interviewed on television it must have been true . As the months passed more and more information was leaked , Billie Piper was being cast , the Daleks would be returning and The Mill , the Hollywood effects company who had done the FX for GLADIATOR were contracted to do the special effects for the show . For several weeks before the first broadcast trailers galore heralded the return of the new series , massive billboards in London informed the public about the return of the show , tabloid newspapers carried massive photo spreads of the aliens appearing and Christopher Eccleston appeared on programmes as diverse as BLUE PETER , MASTERMIND ( Which had a special DOCTOR WHO night edition ) , THIS MORNING and Friday NIGHT WITH JOHNATHAN ROSS . In fact this new series of DOCTOR WHO must have been the most hyped programme in the history of British television , it had better be bloody good <br /><br />So was it bloody good ? Undoubtedly it has been a major success with nearly every episode making the top ten shows in the TV charts . To give you clue of its rating success only one episode ( The Ark In Space episode two - Febuary 1975 ) from the old series had made it into the top five TV chart . The opening series episode made number three with two more episodes either beating or equalling the previous record and this is in an era where there's far more competition in terms of TV stations and choice . Let's laugh and cheer at the fact DOCTOR WHO stuffed HIT ME BABY ONE MORE TIME , CELEBRITY WRESTLING and mauled ANT AND DEC'S Saturday NIGHT TAKEAWAY . Of course much of the success is down to the breath taking visuals and the casting of a well known prestigious actor in the role . For the most part everything you see on screen here equals anything you'll see in a Spielberg / Hollywood movie . There's a Dalek invasion force numbering tens of thousands , exotic aliens , a 19th Century Cardiff that looks like a 19th Century Cardiff and night filming that is actually night filming and not done by sticking a dark filter over the screen . I promise you'll be hearing a lot more from the directors who worked on this series , Joe Ahearne especially will one day be in the Hollywood A list <br /><br />There are some flaws to the new series of DOCTOR WHO and all of them should be laid at the door of Russell T Davies . It may be contentious whether the soap opera and post modernist elements are successful or not ( In my opinion they're not ) but what's not in dispute is that the weakest scripts are all written by RTD . As I mentioned in my review of CASANOVA he cheats the audience and he does the same thing here: when faced by armed soldiers pointing their guns at him The Doctor bellows \" attack plan delta \" which makes no sense to anyone in the audience but allows him to escape from a tight spot , a naked Captain Jack suddenly pulls out a laser he's been hiding and RTD scripts are full of these type of cheats and deus ex machina type endings . In fact the final episode is spoiled greatly by the ridiculous concept of what the \" Bad Wolf \" is which seems to have got RTD out of a tight spot more than The Doctor . And of the endings I'm trying to remember if any of them were actually down to The Doctor ? More often than it's a supporting character or the Doctor's companion who saves the day . The show is called DOCTOR WHO not ROSE TYLER so can we see the title character save the day please just like he did in the classic series ? One final point about the portrayal of the Doctor is the way he's written as a grinning loon . Eccleston is best [MASK] for his serious and gloomy roles and he's absolutely breath taking at scenes when he's showing grief , like the tear running down his face in the End Of The World but more often than not he's written as a \" Tom Baker on speed \" character . It's obvious why Eccleston hasn't done much comedy in his career - He's not very good at it <br /><br />Am I starting to sound like I hate this show ? Sorry I didn't mean to but it's just that while some anticipations have been met or surpassed some others haven't and they're nearly all down to Russell T Davies who thankfully is contributing less in the way of scripts in the next series of DOCTOR WHO . Let's see more traditional stories of a human outpost being under threat from monsters like we saw in the 1960s and 70s , imagine a story like The Sea Devils with a massive budget directed by Joe Ahearne ! Oh and one last request - Can we see these \" NEXT TIME \" trailers scrapped ? They reveal all the best bits of next week's episode\n",
            "this is awesome!!! there is no partnership quite like Errol, and Olivia. there love is genuine! I'm 24, yet this flick is as captivating now as I'm sure it was 60 years ago. Raoul Walsh is an under-rated genius, his direction is so sweeping, so broad, yet so intimate. the last scene between colonel custer (Flynn), and his wife (de havilland), almost brought me to tears (Not easy for a 24yr old guy!!), its so heart-wrenching. there is also a deep Christian message implicit here, the faith Custer has in taking your glory with you, and the trust, and fidelity of his wife to the extent of letting him go, in order that he fulfils his moral [MASK] to protect the innocent civilians from certain massacre. there is no movie that deals with these issues quite like this. a must-see for anyone who wants to look at this defining moment in American, and military history, from the inside. patriotic, for all the right reasons. i knew Errol Flynn was a star, and De havilland was a screen legend-this only confirms my suspicions that they are among the very greatest!\n",
            "Following his role in the fine caper SEVEN THIEVES (1960) – which I’ve watched several years back – Edward G. Robinson seemed to be stuck playing elderly criminal mastermind types (apart from the odd juicy role as in THE CINCINNATI KID [1965]). I’d previously watched the pretty good “Euro-Cult” effort GRAND SLAM (1967) and, apart from this, I’ve yet two more similar titles from Italy to check – one of which was directed by future goremeister Lucio Fulci! Anyway, this is the kind of international production – featuring American and Italian actors and a British director – which was prevalent during the 1960s; it’s harmless and easy-going in itself but hardly memorable and definitely overlong – especially since to procure finance for the heavy-duty equipment required for the heist (such as an army tank and an airplane!), the gang involved have to pull a variety of minor thefts first.<br /><br />The gang, of course, is an incompetent lot led by an American (Robert Wagner) and his bimbo girlfriend (Raquel Welch) – the others are a ‘pacifist’ black man, a perennially hungry Italian and a diminutive Englishman. They try to induce an ex-gangster (Vittorio De Sica) to turn over his fortune to them, except he’s destitute…but, under the auspices of “Professor” Robinson, he proposes instead a caper of 5 million dollars’ worth of platinum! Needless to say, the gang members don’t trust one another (Wagner instructs Welch to seduce De Sica so as to get the name of their fence in Morocco – where they are to retreat after the robbery), or else bungle the job (commissioned to hold up a restaurant, the Italian can’t resist sitting at table and order a multi-course meal for himself!). Amusingly, in the face of similar failures, De Sica tries to show them how they used to do it in the old days – however, ostensibly holding up a petrol station, it transpires that the owner is a nephew of his and he merely asked to borrow some cash! <br /><br />The central heist sequence is typically elaborate: while the gang, including Welch, ‘take’ the train transporting the platinum, Wagner kidnaps pilot Victor Spinetti and his airplane. When the job is done, he fully intends to double-cross De Sica – but neither his partners nor Welch herself are willing to go along with this, so he’s forced to relent. Coming from the [MASK] when crime didn’t pay, the gang contrives to lose all their stash in mid-air when the plane’s bomb-bay doors are accidentally opened…\n",
            "An expedition party made up of constantly bickering and obnoxious jerks go trekking into the dangerous African jungle in search of both a fortune in diamonds and a missing young lady named Diana (luscious brunette looker Katja Biernet, clad solely in a skimpy loincloth that shows off a lot of her hot shapely body) who's worshiped as a goddess by a deadly primitive tribe called the Mabutos. Director/screenwriter Jess Franco crucially fails to inject any style or vigor into the generally blah and meandering proceedings, allowing the sluggish pace to crawl along at an often agonizingly slow clip and staging the infrequent action scenes with a singular lack of skill and panache. The lousy dubbing, excess amount of grainy \"National Geographic\"-like animal stock footage, groovy, jazzy lounge score, terrible acting, talky, uneventful narrative, tepid soft-core sex scenes, and static photography don't help matters any as well. Fortunately, there's plenty of tasty gratuitous nudity on sight to alleviate the tedium to a reasonable extent: Besides the delectable Biernert, both Aline Mess as fierce, wicked high priestess Noba and Mari Carmen Nieto as the conniving, treacherous Lita are likewise real easy on the eyes. The beautiful jungle scenery is very nice, too. But overall this picture sizes up as barely watchable [MASK] hence instantly forgettable swill.\n",
            "\n",
            "Correct Answers:\n",
            "couple\n",
            "\"funny\".\n",
            "was\n",
            "state\n",
            "hunt\n",
            "that\n",
            "known\n",
            "duty\n",
            "time\n",
            "and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datasets import load_dataset\n",
        "comments = dataset['test']['text']\n",
        "\n",
        "random_comment = random.choice(comments)\n",
        "words = random_comment.split()\n",
        "mask_index = random.randint(1, len(words) - 2)\n",
        "correct_answer = words[mask_index]\n",
        "\n",
        "words[mask_index] = \"[MASK]\"\n",
        "masked_comment = \" \".join(words)\n",
        "print(\"Masked:\")\n",
        "print(masked_comment)\n",
        "print(\"Correct:\")\n",
        "print(correct_answer)"
      ],
      "metadata": {
        "id": "YU1aqEdnq1qc",
        "outputId": "ee7fb94b-84af-4c00-9900-c0e15e6fba11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masked:\n",
            "The acting in this film was of the old school: corny and stiff. Irene Dunne is luminous, and comes off the best even though she has some very unnatural lines to say. Still, her ability to [MASK] emotion comes through.<br /><br />Old movie buffs will find at least some redeeming qualities in this film through observation of cinematic technique of the 1930s. Otherwise, it is not really that worthwhile.\n",
            "Correct:\n",
            "convey\n"
          ]
        }
      ]
    }
  ]
}